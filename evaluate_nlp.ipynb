{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jieba\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chinese_tokenize(text):\n",
    "    return list(jieba.cut(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(reference, hypothesis):\n",
    "    smooth_fn = SmoothingFunction().method1\n",
    "    \n",
    "    reference = chinese_tokenize(reference)\n",
    "    hypothesis = chinese_tokenize(hypothesis)\n",
    "    \n",
    "    bleu1 = sentence_bleu([reference], hypothesis, weights=(1, 0, 0, 0), smoothing_function=smooth_fn)\n",
    "    bleu2 = sentence_bleu([reference], hypothesis, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth_fn)\n",
    "    bleu3 = sentence_bleu([reference], hypothesis, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth_fn)\n",
    "    bleu4 = sentence_bleu([reference], hypothesis, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth_fn)\n",
    "    \n",
    "    return {\n",
    "        \"bleu1\": bleu1,\n",
    "        \"bleu2\": bleu2,\n",
    "        \"bleu3\": bleu3,\n",
    "        \"bleu4\": bleu4\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge(reference, hypothesis):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    reference = ' '.join(chinese_tokenize(reference))\n",
    "    hypothesis = ' '.join(chinese_tokenize(hypothesis))\n",
    "    \n",
    "    scores = scorer.score(reference, hypothesis)\n",
    "    \n",
    "    return {\n",
    "        \"rouge-1\": scores['rouge1'],\n",
    "        \"rouge-2\": scores['rouge2'],\n",
    "        \"rouge-l\": scores['rougeL']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_outputs(input_file, output_file):\n",
    "    reference_data = read_data(input_file)\n",
    "    generated_data = read_data(output_file)\n",
    "\n",
    "    bleu_scores = []\n",
    "    rouge_scores = []\n",
    "\n",
    "    if len(reference_data) != len(generated_data):\n",
    "        raise ValueError(\"参考数据与生成数据长度不一致！\")\n",
    "    \n",
    "    for ref_instance, gen_instance in zip(reference_data, generated_data):  \n",
    "        reference_output = ref_instance.get(\"output\")\n",
    "        generated_output = gen_instance.get(\"output\")\n",
    "        \n",
    "        if reference_output and generated_output:\n",
    "            bleu = calculate_bleu(reference_output, generated_output)\n",
    "            bleu_scores.append(bleu)\n",
    "            \n",
    "            rouge = calculate_rouge(reference_output, generated_output)\n",
    "            rouge_scores.append(rouge)\n",
    "    \n",
    "    bleus = {\n",
    "        \"bleu1\": sum([b[\"bleu1\"] for b in bleu_scores]) / len(bleu_scores),\n",
    "        \"bleu2\": sum([b[\"bleu2\"] for b in bleu_scores]) / len(bleu_scores),\n",
    "        \"bleu3\": sum([b[\"bleu3\"] for b in bleu_scores]) / len(bleu_scores),\n",
    "        \"bleu4\": sum([b[\"bleu4\"] for b in bleu_scores]) / len(bleu_scores)\n",
    "    }\n",
    "\n",
    "    rouges = {\n",
    "        \"rouge-1\": sum([r[\"rouge-1\"].fmeasure for r in rouge_scores]) / len(rouge_scores),\n",
    "        \"rouge-2\": sum([r[\"rouge-2\"].fmeasure for r in rouge_scores]) / len(rouge_scores),\n",
    "        \"rouge-l\": sum([r[\"rouge-l\"].fmeasure for r in rouge_scores]) / len(rouge_scores)\n",
    "    }\n",
    "\n",
    "    print(\"\\nBLEU Scores:\")\n",
    "    print(f\"BLEU-1: {bleus['bleu1']*100:.2f}, BLEU-2: {bleus['bleu2']*100:.2f}, BLEU-3: {bleus['bleu3']*100:.2f}, BLEU-4: {bleus['bleu4']*100:.2f}\")\n",
    "\n",
    "    print(\"\\nROUGE Scores:\")\n",
    "    print(f\"ROUGE-1: {rouges['rouge-1']*100:.2f}, ROUGE-2: {rouges['rouge-2']*100:.2f}, ROUGE-L: {rouges['rouge-l']*100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = 'dataset/cMKGQA/evaluation_data.json'\n",
    "output_file_path = 'output/cMKGQA/test_medka-8b.json'\n",
    "\n",
    "evaluate_outputs(input_file_path, output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
